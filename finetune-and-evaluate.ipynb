{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Getting started with CrystaLLM**\n",
    "To setup local enviornment, following the **'Getting Started'** of the README.md\n",
    "\n",
    "We need a clean env with dependencies installed and the model in editable mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda create --yes --quiet --name ft_crystallm_venv python=3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch==2.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Formatting your training data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To exemplify data processing steps for the package, we'll be using the **JARVIS-DFT** dataset (https://www.nature.com/articles/s41524-020-00440-1). It contains various 3D materials computed with OptB88vdW and TBmBJ methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install jarvis-tools to get the JARVIS-DFT dataset\n",
    "%pip install jarvis-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to generate a folder that contains .cif files. The preferred formatting is of pymatgen's CifWriter - as this was the one used to generate the original model's training data. This can be done by generating a pymatgen structure from a material then converting it to a CIF file. Here lets make a small 1K dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the JARVIS-DFT dataset and get CIFs for a random 1K samples\n",
    "!python finetune_example_scripts/generate_jarvis_data.py \\\n",
    "    --output_folder 'finetune_example/data_formatting/jarvis_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we preprocess the files as per the **'Using Your Own CIF Files'** in the README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing CIF files...: 100%|███████████████| 1000/1000 [00:23<00:00, 41.68it/s]\n",
      "prepared CIF files have been saved to finetune_example/data_formatting/jarvis_data.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# convert the data to a single tar.gz file\n",
    "!python bin/prepare_custom.py \\\n",
    "    finetune_example/data_formatting/jarvis_data/ \\\n",
    "    finetune_example/data_formatting/jarvis_data.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from finetune_example/data_formatting/jarvis_data.tar.gz...\n",
      "extracting files...: 100%|███████████████| 1000/1000 [00:00<00:00, 86537.59it/s]\n",
      "saving data to finetune_example/data_formatting/jarvis_data.pkl.gz...\n",
      "conversion complete!\n"
     ]
    }
   ],
   "source": [
    "# convert the data to pickle format\n",
    "!python bin/tar_to_pickle.py \\\n",
    "    finetune_example/data_formatting/jarvis_data.tar.gz \\\n",
    "    finetune_example/data_formatting/jarvis_data.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from finetune_example/data_formatting/jarvis_data.pkl.gz...\n",
      "number of CIFs to deduplicate: 1,000\n",
      "100%|███████████████████████████████████| 1000/1000 [00:00<00:00, 114956.53it/s]\n",
      "number of entries to write: 996\n",
      "saving data to finetune_example/data_formatting/jarvis_data_dedup.pkl.gz...\n"
     ]
    }
   ],
   "source": [
    "# deduplicate the data\n",
    "!python bin/deduplicate.py \\\n",
    "    finetune_example/data_formatting/jarvis_data.pkl.gz \\\n",
    "    --out finetune_example/data_formatting/jarvis_data_dedup.pkl.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from finetune_example/data_formatting/jarvis_data_dedup.pkl.gz...\n",
      "100%|███████████████████████████████████████| 996/996 [00:00<00:00, 2260.71it/s]\n",
      "number of CIFs: 996\n",
      "saving data to finetune_example/data_formatting/jarvis_data_prep.pkl.gz...\n"
     ]
    }
   ],
   "source": [
    "# preprocess the data, here we are reformatting the cif files as per the model requirements\n",
    "!python bin/preprocess.py \\\n",
    "    finetune_example/data_formatting/jarvis_data_dedup.pkl.gz \\\n",
    "    --out finetune_example/data_formatting/jarvis_data_prep.pkl.gz \\\n",
    "    --workers 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from finetune_example/data_formatting/jarvis_data_prep.pkl.gz...\n",
      "splitting dataset...\n",
      "number of CIFs in train set: 851\n",
      "number of CIFs in validation set: 95\n",
      "number of CIFs in test set: 50\n",
      "writing train set...\n",
      "writing validation set...\n",
      "writing test set...\n"
     ]
    }
   ],
   "source": [
    "# split the data into train, val and test sets\n",
    "!python bin/split.py \\\n",
    "    finetune_example/data_formatting/jarvis_data_prep.pkl.gz \\\n",
    "    --train_out finetune_example/data_formatting/jarvis_data_train.pkl.gz \\\n",
    "    --val_out finetune_example/data_formatting/jarvis_data_val.pkl.gz \\\n",
    "    --test_out finetune_example/data_formatting/jarvis_data_test.pkl.gz \\\n",
    "    --test_size 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from finetune_example/data_formatting/jarvis_data_train.pkl.gz...\n",
      "loading data from finetune_example/data_formatting/jarvis_data_train.pkl.gz...\n",
      "preparing files...: 100%|█████████████████| 851/851 [00:00<00:00, 133738.72it/s]\n",
      "preparing files...: 100%|█████████████████| 851/851 [00:00<00:00, 143152.03it/s]\n",
      "tokenizing...: 100%|████████████████████████| 851/851 [00:00<00:00, 3344.84it/s]\n",
      "train min tokenized length: 194\n",
      "train max tokenized length: 1,737\n",
      "train mean tokenized length: 380.57 +/- 152.87\n",
      "train total unk counts: 0\n",
      "tokenizing...: 100%|████████████████████████| 851/851 [00:00<00:00, 1964.74it/s]\n",
      "val min tokenized length: 194\n",
      "val max tokenized length: 1,737\n",
      "val mean tokenized length: 380.57 +/- 152.87\n",
      "val total unk counts: 0\n",
      "concatenating train tokens...: 100%|██████| 851/851 [00:00<00:00, 582656.33it/s]\n",
      "concatenating val tokens...: 100%|████████| 851/851 [00:00<00:00, 677682.31it/s]\n",
      "encoding...\n",
      "train has 323,867 tokens\n",
      "val has 323,867 tokens\n",
      "vocab size: 371\n",
      "exporting to .bin files...\n",
      "creating tar.gz archive...\n",
      "tarball created at jarvis_train_val/jarvis_train_val.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# tokenize the data \n",
    "!python bin/tokenize_cifs.py \\\n",
    "    --train_fname finetune_example/data_formatting/jarvis_data_train.pkl.gz \\\n",
    "    --val_fname finetune_example/data_formatting/jarvis_data_train.pkl.gz \\\n",
    "    --out_dir finetune_example/finetuning/jarvis_train_val/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identifying starts...: 100%|████████| 323867/323867 [00:00<00:00, 505502.96it/s]\n",
      "writing start indices...\n"
     ]
    }
   ],
   "source": [
    "# identify the starting indices of the cifs in the tokenized training data, slightly improves model performance\n",
    "!python bin/identify_starts.py \\\n",
    "    --dataset_fname finetune_example/finetuning/jarvis_train_val/jarvis_train_val.tar.gz \\\n",
    "    --out_fname finetune_example/finetuning/jarvis_train_val/starts.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Finetuning model on new data**\n",
    "\n",
    "To finetune the model on the new data, we need to download a pretrained model. For example, the small or large model can be downloaded. Here we will use the small model, but the logic is the same for any other. This is done as per the **'Using a Pre-trained Model'** in the README.md\n",
    "\n",
    "'The config folder in this project contains a number of model configuration .yaml files. A corresponding .tar.gz model file exists for each .yaml file in that directory that begins with crystallm_, which can be downloaded.' (from README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the pretrained model (small version)\n",
    "!python bin/download.py \\\n",
    "    crystallm_v1_small.tar.gz \\\n",
    "    --out finetune_example/finetuning/\n",
    "\n",
    "# large model can be downloaded with: python bin/download.py crystallm_v1_large.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crystallm_v1_small/\n",
      "crystallm_v1_small/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "# decompress the model\n",
    "!tar xvf finetune_example/finetuning/crystallm_v1_small.tar.gz -C finetune_example/finetuning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to make a config file that will allow us to finetune the model. Most model parameters are adaptable similarly to any DL model. We then want to make sure we set the ***init_from: \"resume\"***, and ***out_dir: 'finetune_example/finetuning/crystallm_v1_small'***\n",
    "\n",
    "_Notes_: \n",
    "- Because the model saves to the same directory as the one it loads the model from, if a finetuned model is saved on top of the original one, you must re-decompress the model as above to fine-tune from the base pre-trained model (here crystallm_v1_small/ckpt.pt)\n",
    "- The **max_iters** argument is set to understand how many iterations the model should train on _in total_. So we look at the config file's max_iters variable for the model we're fine-tuning, then add however many iterations we want to the number stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example config\n",
    "'''\n",
    "out_dir: 'finetune_example/finetuning/crystallm_v1_small'\n",
    "eval_interval: 100  # how often to evaluate against the validation set\n",
    "eval_iters_train: 80\n",
    "eval_iters_val: 80\n",
    "log_interval: 50  # how often to print to the console (1 = every iteration)\n",
    "init_from: 'resume' # for fine-tuning\n",
    "\n",
    "# logging\n",
    "always_save_checkpoint: True  # if set to False, will only save .ckpt if the model improves\n",
    "validate: True  # whether to validate with a validation set\n",
    "\n",
    "# data and batching\n",
    "dataset: 'finetune_example/finetuning/jarvis_train_val'\n",
    "batch_size: 8  # reduce if running out of memory\n",
    "gradient_accumulation_steps: 4  # batch_size * gradient_accumulation_steps = effective batch size\n",
    "\n",
    "# preserve the pre-trained model's parameters\n",
    "block_size: 1024\n",
    "n_layer: 8\n",
    "n_head: 8\n",
    "n_embd: 512\n",
    "dropout: 0.1\n",
    "\n",
    "# editable training parameters (these may need adjustment to specific dataset)\n",
    "learning_rate: 1e-4  # start from lower as the model is already pre-trained\n",
    "decay_lr: True\n",
    "max_iters: 101000  # number of iterations to train for (BASE MODEL IS AT 100K, so add how many more you want to finetune for)\n",
    "lr_decay_iters: 101000  # set to max_iters\n",
    "min_lr: 1e-5 # minimum learning rate (learning_rate/10 usually)\n",
    "warmup_iters: 100  # number of iterations to warm up for\n",
    "beta2: 0.99  # adam parameters\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configuration:\n",
      "out_dir: finetune_example/finetuning/crystallm_v1_small\n",
      "eval_interval: 100\n",
      "log_interval: 50\n",
      "eval_iters_train: 80\n",
      "eval_iters_val: 80\n",
      "eval_only: false\n",
      "always_save_checkpoint: true\n",
      "init_from: resume\n",
      "dataset: finetune_example/finetuning/jarvis_train_val\n",
      "gradient_accumulation_steps: 4\n",
      "batch_size: 8\n",
      "block_size: 1024\n",
      "n_layer: 8\n",
      "n_head: 8\n",
      "n_embd: 512\n",
      "dropout: 0.1\n",
      "bias: false\n",
      "learning_rate: 0.0001\n",
      "max_iters: 101000\n",
      "weight_decay: 0.1\n",
      "beta1: 0.9\n",
      "beta2: 0.99\n",
      "grad_clip: 1.0\n",
      "decay_lr: true\n",
      "warmup_iters: 100\n",
      "lr_decay_iters: 101000\n",
      "min_lr: 1.0e-05\n",
      "device: cuda\n",
      "dtype: bfloat16\n",
      "compile: true\n",
      "underrep_p: 0.0\n",
      "validate: true\n",
      "\n",
      "Creating finetune_example/finetuning/crystallm_v1_small...\n",
      "Reading start indices from finetune_example/finetuning/jarvis_train_val/starts.pkl...\n",
      "Found vocab_size = 371 (inside finetune_example/finetuning/jarvis_train_val/meta.pkl)\n",
      "Resuming training from finetune_example/finetuning/crystallm_v1_small...\n",
      "number of parameters: 25.36M\n",
      "Compiling the model (takes a ~minute)...\n",
      "step 100000: train loss 1.1699, val loss 1.3301\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "[2025-01-20 12:43:36,415] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:36,751] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:37,271] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:37,556] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:37,991] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:38,276] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:38,672] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:38,824] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:39,058] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:39,209] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:39,441] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:39,717] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:39,952] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:40,106] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:40,340] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "[2025-01-20 12:43:40,495] torch._inductor.utils: [WARNING] using triton random, expect difference from eager\n",
      "iter 100000: loss 1.0360, time 14628.48ms, mfu -100.00%\n",
      "iter 100050: loss 0.2696, time 264.43ms, mfu 8.04%\n",
      "step 100100: train loss 0.2199, val loss 0.2602\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 100100: loss 0.2402, time 4194.72ms, mfu 7.29%\n",
      "iter 100150: loss 0.2446, time 265.04ms, mfu 7.36%\n",
      "step 100200: train loss 0.1992, val loss 0.2431\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 100200: loss 0.2191, time 4235.85ms, mfu 6.68%\n",
      "iter 100250: loss 0.2577, time 271.60ms, mfu 6.79%\n",
      "step 100300: train loss 0.1844, val loss 0.2282\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 100300: loss 0.1838, time 4211.73ms, mfu 6.16%\n",
      "iter 100350: loss 0.2127, time 268.96ms, mfu 6.34%\n",
      "step 100400: train loss 0.1748, val loss 0.2255\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 100400: loss 0.2267, time 4070.39ms, mfu 5.76%\n",
      "iter 100450: loss 0.1713, time 272.93ms, mfu 5.96%\n",
      "step 100500: train loss 0.1636, val loss 0.2113\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 100500: loss 0.1719, time 4368.12ms, mfu 5.41%\n",
      "iter 100550: loss 0.1384, time 273.01ms, mfu 5.65%\n",
      "step 100600: train loss 0.1544, val loss 0.2010\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 100600: loss 0.1823, time 4331.76ms, mfu 5.13%\n",
      "iter 100650: loss 0.1663, time 270.02ms, mfu 5.41%\n",
      "step 100700: train loss 0.1418, val loss 0.1993\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 100700: loss 0.1355, time 4242.84ms, mfu 4.92%\n",
      "iter 100750: loss 0.1621, time 274.70ms, mfu 5.20%\n",
      "step 100800: train loss 0.1358, val loss 0.1914\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 100800: loss 0.1221, time 4267.91ms, mfu 4.73%\n",
      "iter 100850: loss 0.1504, time 270.89ms, mfu 5.04%\n",
      "step 100900: train loss 0.1249, val loss 0.1756\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 100900: loss 0.1416, time 4120.89ms, mfu 4.59%\n",
      "iter 100950: loss 0.1188, time 273.89ms, mfu 4.91%\n",
      "step 101000: train loss 0.1222, val loss 0.1733\n",
      "saving checkpoint to finetune_example/finetuning/crystallm_v1_small...\n",
      "iter 101000: loss 0.1217, time 4111.98ms, mfu 4.47%\n"
     ]
    }
   ],
   "source": [
    "!python bin/train.py --config=finetune_example_scripts/jarvis_finetune.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the finetuned model to avoid confusion\n",
    "!mv finetune_example/finetuning/crystallm_v1_small finetune_example/finetuning/crystallm_ft_jarvis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Evaluating model performance**\n",
    "\n",
    "Now that our model has been finetuned, lets evaluate our model. The set can be any CIF dataset, as long as it's fully processed/ as per the first section of the notebook:\n",
    "- In our first example case we will first use the _'finetune_example/data_formatting/jarvis_data_test.pkl.gz'_ generated during preprocessing steps. We will only prompt with the reduced formula, but optionally the space group can be provided.\n",
    "- In the second example we will generate new structures without specifiying the formula ('Ab Initio')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data from finetune_example/data_formatting/jarvis_data_test.pkl.gz...\n",
      "preparing prompts...: 100%|███████████████████| 50/50 [00:00<00:00, 8741.05it/s]\n"
     ]
    }
   ],
   "source": [
    "!python bin/make_prompts.py \\\n",
    "    finetune_example/data_formatting/jarvis_data_test.pkl.gz \\\n",
    "    -o finetune_example/evaluation/prompts_jarvis_test.tar.gz\n",
    "\n",
    "# optionally add\n",
    "# --with-spacegroup (to include spacegroup in the prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the prompts for the test set\n",
    "!tar -xvf finetune_example/evaluation/prompts_jarvis_test.tar.gz -C finetune_example/evaluation/prompts_jarvis_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_Pr1Al2Ni3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets read one of the prompts\n",
    "with open('finetune_example/evaluation/prompts_jarvis_test/JVASP-4720.txt') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When generating the structures, we can specify some generation parameters - a few are discussed here: \n",
    "- **Top-k:** specifies how many of the most probable tokens to consider when generating next token. Higher top-k will give more creative but less coherent outputs, and the inverse for lower top-k.\n",
    "- **Temperature:** also controls creativity of model. Low T (under 1) gives more probability mass to most likely tokens during generation, whereas High T (above 1) makes model predictions more diverse by ditributing probability distribution more evenly over possible next tokens.\n",
    "- **Num-gens:** We can give the model a few attempts to match the 'true' CIF file or specify how many new structures to come up with\n",
    "- **Prompts:** If we want to generate random materials, we can remove the **'--prompts'** argument so the model comes up with a composition itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting prompts...: 100%|█████████████████| 50/50 [00:00<00:00, 60999.19it/s]\n",
      "generating CIFs from prompts...:   0%|                   | 0/50 [00:00<?, ?it/s]initializing model from finetune_example/finetuning/crystallm_ft_jarvis on cuda:0...\n",
      "initializing model from finetune_example/finetuning/crystallm_ft_jarvis on cuda:1...\n",
      "number of parameters: 25.36M\n",
      "number of parameters: 25.36M\n",
      "generating CIFs from prompts...: 100%|██████████| 50/50 [04:11<00:00,  5.03s/it]\n",
      "writing CIF files to finetune_example/evaluation/gen_jarvis_test.tar.gz...: 100%\n"
     ]
    }
   ],
   "source": [
    "# lets evaluate the model on test set\n",
    "!python bin/generate_cifs.py \\\n",
    "    --model finetune_example/finetuning/crystallm_ft_jarvis \\\n",
    "    --prompts finetune_example/evaluation/prompts_jarvis_test.tar.gz \\\n",
    "    --out finetune_example/evaluation/gen_jarvis_test.tar.gz \\\n",
    "    --device cuda \\\n",
    "    --num-gens 20 \\\n",
    "    --top-k 10 \\\n",
    "    --temperature 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating CIFs from prompts...:   0%|                   | 0/20 [00:00<?, ?it/s]initializing model from finetune_example/finetuning/crystallm_ft_jarvis on cuda:1...\n",
      "initializing model from finetune_example/finetuning/crystallm_ft_jarvis on cuda:0...\n",
      "number of parameters: 25.36M\n",
      "number of parameters: 25.36M\n",
      "generating CIFs from prompts...: 100%|██████████| 20/20 [00:09<00:00,  2.11it/s]\n",
      "writing CIF files to finetune_example/evaluation/gen_jarvis_ab_initio.tar.gz...:\n"
     ]
    }
   ],
   "source": [
    "# lets evaluate the model without specifiying any formulae\n",
    "!python bin/generate_cifs.py \\\n",
    "    --model finetune_example/finetuning/crystallm_ft_jarvis \\\n",
    "    --out finetune_example/evaluation/gen_jarvis_ab_initio.tar.gz \\\n",
    "    --device cuda \\\n",
    "    --num-gens 20 \\\n",
    "    --top-k 10 \\\n",
    "    --temperature 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the generated cifs for test set\n",
    "!tar -xvf finetune_example/evaluation/gen_jarvis_test.tar.gz -C finetune_example/evaluation/gen_jarvis_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_Pr1Al2Ni3\n",
      "loop_\n",
      "_atom_type_symbol\n",
      "_atom_type_electronegativity\n",
      "_atom_type_radius\n",
      "_atom_type_ionic_radius\n",
      "Pr 1.1300 1.8500 1.0600\n",
      "Al 1.6100 1.2500 0.6750\n",
      "Ni 1.9100 1.3500 0.7400\n",
      "_symmetry_space_group_name_H-M P6/mmm\n",
      "_cell_length_a 5.3231\n",
      "_cell_length_b 5.3231\n",
      "_cell_length_c 3.8552\n",
      "_cell_angle_alpha 90.0000\n",
      "_cell_angle_beta 90.0000\n",
      "_cell_angle_gamma 120.0000\n",
      "_symmetry_Int_Tables_number 191\n",
      "_chemical_formula_structural PrAl2Ni3\n",
      "_chemical_formula_sum 'Pr1 Al2 Ni3'\n",
      "_cell_volume 94.5961\n",
      "_cell_formula_units_Z 1\n",
      "loop_\n",
      "_symmetry_equiv_pos_site_id\n",
      "_symmetry_equiv_pos_as_xyz\n",
      "1 'x, y, z'\n",
      "loop_\n",
      "_atom_site_type_symbol\n",
      "_atom_site_label\n",
      "_atom_site_symmetry_multiplicity\n",
      "_atom_site_fract_x\n",
      "_atom_site_fract_y\n",
      "_atom_site_fract_z\n",
      "_atom_site_occupancy\n",
      "Pr Pr0 1 0.0000 0.0000 0.0000 1.0\n",
      "Al Al1 2 0.3333 0.6667 0.0000 1.0\n",
      "Ni Ni2 3 0.0000 0.5000 0.5000 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets look at one of the generated cifs for test set\n",
    "with open('finetune_example/evaluation/gen_jarvis_test/JVASP-4720__1.cif') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the generated cifs for ab initio\n",
    "!tar -xvf finetune_example/evaluation/gen_jarvis_ab_initio.tar.gz -C finetune_example/evaluation/gen_jarvis_ab_initio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_Li12Mn6V18O48\n",
      "loop_\n",
      "_atom_type_symbol\n",
      "_atom_type_electronegativity\n",
      "_atom_type_radius\n",
      "_atom_type_ionic_radius\n",
      "Li 0.9800 1.4500 0.9000\n",
      "Mn 1.5500 1.4000 0.6483\n",
      "V 1.6300 1.3500 0.7775\n",
      "O 3.4400 0.6000 1.2600\n",
      "_symmetry_space_group_name_H-M R-3m\n",
      "_cell_length_a 5.7953\n",
      "_cell_length_b 5.7953\n",
      "_cell_length_c 28.5364\n",
      "_cell_angle_alpha 90.0000\n",
      "_cell_angle_beta 90.0000\n",
      "_cell_angle_gamma 120.0000\n",
      "_symmetry_Int_Tables_number 166\n",
      "_chemical_formula_structural Li2MnV3O8\n",
      "_chemical_formula_sum 'Li12 Mn6 V18 O48'\n",
      "_cell_volume 829.9055\n",
      "_cell_formula_units_Z 6\n",
      "loop_\n",
      "_symmetry_equiv_pos_site_id\n",
      "_symmetry_equiv_pos_as_xyz\n",
      "1 'x, y, z'\n",
      "loop_\n",
      "_atom_site_type_symbol\n",
      "_atom_site_label\n",
      "_atom_site_symmetry_multiplicity\n",
      "_atom_site_fract_x\n",
      "_atom_site_fract_y\n",
      "_atom_site_fract_z\n",
      "_atom_site_occupancy\n",
      "Li Li0 6 0.0000 0.0000 0.1849 1.0\n",
      "Li Li1 6 0.0000 0.0000 0.3110 1.0\n",
      "Mn Mn2 3 -0.0000 -0.0000 0.5000 1.0\n",
      "Mn Mn3 3 0.0000 0.0000 0.0000 1.0\n",
      "V V4 18 0.0109 0.5054 0.2493 1.0\n",
      "O O5 18 0.0244 0.5122 0.1252 1.0\n",
      "O O6 18 0.0343 0.5172 0.6229 1.0\n",
      "O O7 6 0.0000 0.0000 0.1181 1.0\n",
      "O O8 6 0.0000 0.0000 0.3795 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets look at one of the generated cifs for test set\n",
    "with open('finetune_example/evaluation/gen_jarvis_ab_initio/1__1.cif') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets evaluate the generated CIFs, the results include:\n",
    "- The fraction of generated CIF files where the printed space group is consistent with the generated structure\n",
    "- The fraction of generated CIF files that are consistent in terms of atom site multiplicity\n",
    "- The average bond length reasonableness score, and the fraction of generated CIF files that have reasonable bond lengths\n",
    "- The fraction of generated CIF files that are valid\n",
    "- The longest valid generated tokenized length\n",
    "- The average valid generated tokenized length\n",
    "\n",
    "A few sensibility parameters can be specified during evaluation (check bin/evaluate_cifs.py args) including:\n",
    "- The smallest or largest cell length allowable\n",
    "- The smallest or largest cell angle allowable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting generated CIFs...: 100%|█████| 1000/1000 [00:00<00:00, 144158.93it/s]\n",
      "100%|███████████████████████████████████████| 1000/1000 [04:06<00:00,  4.05it/s]\n",
      "space group consistent: 970/1000 (0.970)\n",
      "atom site multiplicity consistent: 966/1000 (0.966)\n",
      "avg. bond length reasonableness score: 0.9660 ± 0.1270\n",
      "bond lengths reasonable: 842/1000 (0.842)\n",
      "num valid: 837/1000 (0.84)\n",
      "longest valid generated length: 577\n",
      "avg. valid generated length: 333.119 ± 55.803\n"
     ]
    }
   ],
   "source": [
    "# Lets evaluate the generated cifs for the test set\n",
    "!python bin/evaluate_cifs.py \\\n",
    "    finetune_example/evaluation/gen_jarvis_test.tar.gz \\\n",
    "    -o finetune_example/results/jarvis_test_results.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting generated CIFs...: 100%|██████████| 20/20 [00:00<00:00, 37449.14it/s]\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]ERROR: 'NoneType' object is not subscriptable\n",
      " 25%|███████████                                 | 5/20 [00:02<00:06,  2.15it/s]ERROR: \n",
      " 40%|█████████████████▌                          | 8/20 [00:02<00:02,  4.61it/s]ERROR: 'NoneType' object is not subscriptable\n",
      "100%|███████████████████████████████████████████| 20/20 [00:05<00:00,  3.99it/s]\n",
      "space group consistent: 17/20 (0.850)\n",
      "atom site multiplicity consistent: 19/20 (0.950)\n",
      "avg. bond length reasonableness score: 0.9676 ± 0.0909\n",
      "bond lengths reasonable: 15/20 (0.750)\n",
      "num valid: 15/20 (0.75)\n",
      "longest valid generated length: 874\n",
      "avg. valid generated length: 441.200 ± 162.956\n"
     ]
    }
   ],
   "source": [
    "# Lets evaluate the generated cifs ab initio\n",
    "!python bin/evaluate_cifs.py \\\n",
    "    finetune_example/evaluation/gen_jarvis_ab_initio.tar.gz \\\n",
    "    -o finetune_example/results/jarvis_ab_initio_results.csv \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the wanted application, it may be useful to save all valid CIFs to a new folder, so that they can be processed into standard format CIFs after. This can be done using the **'finetune_example_scripts/evaluate_cifs_custom.py'**. The custom scipt preserves the same logic as above, but it also saves the valid cifs to a new folder if the **'--save_valid_dir'** is specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting generated CIFs...: 100%|██████████| 20/20 [00:00<00:00, 65484.84it/s]\n",
      "100%|███████████████████████████████████████████| 20/20 [00:05<00:00,  3.96it/s]\n",
      "space group consistent: 17/20 (0.850)\n",
      "atom site multiplicity consistent: 19/20 (0.950)\n",
      "avg. bond length reasonableness score: 0.9676 ± 0.0909\n",
      "bond lengths reasonable: 15/20 (0.750)\n",
      "num valid: 15/20 (0.75)\n",
      "longest valid generated length: 874\n",
      "avg. valid generated length: 441.200 ± 162.956\n"
     ]
    }
   ],
   "source": [
    "# Lets evaluate the generated cifs ab initio\n",
    "!python finetune_example_scripts/evaluate_cifs_custom.py \\\n",
    "    finetune_example/evaluation/gen_jarvis_ab_initio.tar.gz \\\n",
    "    -o finetune_example/results/jarvis_ab_initio_results.csv \\\n",
    "    --save_valid_dir finetune_example/results/jarvis_ab_initio_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then want to convert the generated valid CIFs to useable and standard format CIF files, we can process the valid structures using the bin/postprocess.py. Lets perform this on the valid ab initio generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed: Li10Mn2Co4O16.cif\n",
      "processed: Yb4B16Rh4.cif\n",
      "processed: Li4V3Cr2O10.cif\n",
      "processed: Na2Nd6Ti4Sb4O28.cif\n",
      "processed: Ba4Ga4Se8.cif\n",
      "processed: Li4Ti4Cr4O16.cif\n",
      "processed: Li8Fe4P8O32.cif\n",
      "processed: Mg12Ti2Sb2.cif\n",
      "processed: K2Ag1Hg1.cif\n",
      "processed: Mn1Cu3S4.cif\n",
      "processed: Mn1Sn3.cif\n",
      "processed: Be1Cu1As1.cif\n",
      "processed: Ca4Mn4Zn4.cif\n",
      "processed: Rb2Sc2Te2O2.cif\n",
      "processed: K8Sb4Au4Cl24.cif\n"
     ]
    }
   ],
   "source": [
    "!python bin/postprocess.py \\\n",
    "    finetune_example/results/jarvis_ab_initio_valid \\\n",
    "    finetune_example/results/jarvis_ab_initio_postprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_Ba4Ga4Se8\n",
      "_symmetry_space_group_name_H-M Pnma\n",
      "_cell_length_a 7.1261\n",
      "_cell_length_b 4.7277\n",
      "_cell_length_c 13.2755\n",
      "_cell_angle_alpha 90.0000\n",
      "_cell_angle_beta 90.0000\n",
      "_cell_angle_gamma 90.0000\n",
      "_symmetry_Int_Tables_number 62\n",
      "_chemical_formula_structural BaGaSe2\n",
      "_chemical_formula_sum 'Ba4 Ga4 Se8'\n",
      "_cell_volume 449.9548\n",
      "_cell_formula_units_Z 4\n",
      "loop_\n",
      " _symmetry_equiv_pos_site_id\n",
      " _symmetry_equiv_pos_as_xyz\n",
      "  1  '-x, y+1/2, -z'\n",
      "  2  '-x, -y, -z'\n",
      "  3  '-x+1/2, -y, z+1/2'\n",
      "  4  'x+1/2, -y+1/2, -z+1/2'\n",
      "  5  'x, y, z'\n",
      "  6  'x-1/2, y, -z-1/2'\n",
      "  7  '-x-1/2, y-1/2, z-1/2'\n",
      "  8  'x, -y-1/2, z'\n",
      "loop_\n",
      "_atom_site_type_symbol\n",
      "_atom_site_label\n",
      "_atom_site_symmetry_multiplicity\n",
      "_atom_site_fract_x\n",
      "_atom_site_fract_y\n",
      "_atom_site_fract_z\n",
      "_atom_site_occupancy\n",
      "Ba Ba0 4 0.2116 0.2500 0.1344 1.0\n",
      "Ga Ga1 4 0.2376 0.2500 0.8763 1.0\n",
      "Se Se2 4 0.0347 0.2500 0.7377 1.0\n",
      "Se Se3 4 0.2489 0.7500 0.9374 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets check the postprocessed cifs for ab initio generation\n",
    "with open('finetune_example/results/jarvis_ab_initio_postprocessed/Ba4Ga4Se8.cif') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These materials can then be used down the line for other tasks. A few functionalities were not covered in this notebook such as Monte Carlo Tree Search Decoding, single formula prompt generation, extractiong learned embeddings... These can all be found in the **README.md**.\n",
    "\n",
    "For any additional queries, feel free to e-mail cyprien.bone.24@ucl.ac.uk or k.t.butler@ucl.ac.uk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft_crystallm_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
